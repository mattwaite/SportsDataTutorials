---
title: "Sports Data Lesson 35: Clustering"
output: 
  learnr::tutorial:
    progressive: true
    allow_skip: true
    theme: united
    ace_theme: github
runtime: shiny_prerendered
description: >
  Learn how to group like things together with math.
---

```{r setup, include=FALSE}
library(learnr)
library(gradethis)
library(tidyverse)
library(cluster)
knitr::opts_chunk$set(echo = FALSE)
tutorial_options(exercise.completion=FALSE)
```

## The Goal

The goal of this lesson is to introduce you to k-means clustering, a powerful technique for grouping similar data points in sports analytics. By the end of this tutorial, you'll understand how to use clustering to identify player types and find comparable athletes based on their statistical profiles. We'll walk through the process of preparing data, determining the optimal number of clusters, and interpreting the results using college basketball player statistics. You'll learn how to apply k-means clustering to group players with similar performance metrics, visualize the clusters, and use this information to find peers for specific athletes like Keisei Tominaga. This skill will enhance your ability to conduct more sophisticated player comparisons and team composition analyses, providing valuable insights for talent evaluation and strategic decision-making in sports.

## The Basics

One common effort in sports is to classify teams and players -- who are this player's peers? What teams are like this one? Who should we compare a player to? Truth is, most sports commentators use nothing more sophisticated that looking at a couple of stats or use the "eye test" to say a player is like this or that. 

There's better ways. 

In this lesson, we're going to use a method that sounds advanced but it really quite simple called k-means clustering. It's based on the concept of the k-nearest neighbor algorithm. You're probably already scared. Don't be. 

Imagine two dots on a scatterplot. If you took a ruler out and measured the distance between those dots, you'd know how far apart they are. In math, that's called the Euclidean distance. It's just the space between them in numbers. Where k-nearest neighbor comes in, you have lots of dots and you want measure the distance between all of them. What does k-means clustering do? It lumps them into groups based on the average distance between them. Players who are good on offense but bad on defense are over here, good offense good defense are over here. And using the Euclidean distance between them, we can decide who is in and who is out of those groups.

For this exercise, I want to look at Keisei Tominaga, probably the most fun player ever on a Fred Hoiberg Nebraska team. So who does Keisei compare to? 

To answer this, we'll use k-means clustering. 

First thing we do is load some libraries and set a seed, so if we run this repeatedly, our random numbers are generated from the same base. If you don't have the cluster library, just add it on the console with `install.packages("cluster")`

```{r load-tidyverse, exercise=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
library(cluster)

set.seed(1234)
```
```{r load-tidyverse-solution}
library(tidyverse)
library(cluster)

set.seed(1234)
```
```{r load-tidyverse-check}
grade_this_code()
```

Let's load the player data from this season. Here it is if you want to try this in your own notebook.

```{r echo=FALSE, class.output="bg-info", results="asis",  message=FALSE,  warning=FALSE}
library(downloadthis)
library(glue)

dllink <- download_link(
  link = "https://mattwaite.github.io/sportsdatafiles/players24.csv",
  button_label = "Download csv file",
  button_type = "danger",
  has_icon = TRUE,
  icon = "fa fa-save",
  self_contained = FALSE
)

glue("<pre><p><strong>For this walkthrough:</strong></p><p>{dllink}</p></pre>")
```

Now load that data.

```{r cluster-load-data, message=FALSE, warning=FALSE}
set.seed(1234)

players <- read_csv("https://mattwaite.github.io/sportsdatafiles/players24.csv")

playersselected <- players |> 
  filter(MP>0) |> filter(Pos == "G") |> 
  select(Player, Team, Pos, MP, `FG%`, `3P%`, AST, TOV, PTS) |> 
  na.omit() 

playersscaled <- playersselected |> 
  select(MP, `FG%`, `3P%`, AST, TOV, PTS) |> 
  mutate_all(scale) |> 
  na.omit()

clusters <- kmeans(playersscaled, centers = 5, nstart = 25)

playercluster <- data.frame(playersselected, clusters$cluster) 

kt <- playercluster |> filter(Player == "Keisei Tominaga")
```
```{r cluster-load-data-exercise, exercise = TRUE}
players <- read_csv("https://mattwaite.github.io/sportsdatafiles/players24.csv")
```
```{r cluster-load-data-exercise-solution}
players <- read_csv("https://mattwaite.github.io/sportsdatafiles/players24.csv")
```
```{r cluster-load-data-exercise-check}
grade_this_code()
```

To cluster this data properly, we have some work to do.

First, it won't do to have players who haven't played, so we can use filter to find anyone with greater than 0 minutes played. Next, Keisei Tominaga is a guard, so let's just look at guards. Third, we want to limit the data to things that make sense to look at for Keisei Tominaga -- things like shooting, three point shooting, assists, turnovers and points. 

```{r cluster1, exercise=TRUE, exercise.setup = "cluster-load-data", message=FALSE}
playersselected <- players |> 
  filter(MP>0) |> filter(Pos == "G") |> 
  select(Player, Team, Pos, MP, `FG%`, `3P%`, AST, TOV, PTS) |> 
  na.omit() 
```
```{r cluster1-solution, exercise.reveal_solution = FALSE}
playersselected <- players |> 
  filter(MP>0) |> filter(Pos == "G") |> 
  select(Player, Team, Pos, MP, `FG%`, `3P%`, AST, TOV, PTS) |> 
  na.omit() 
```
```{r cluster1-check}
grade_this_code()
```

Now, k-means clustering doesn't work as well with data that can be on different scales. So comparing a percentage to a count metric -- shooting percentage to points -- would create chaos because shooting percentages are a fraction of 1 and points, depending on when they are in the season, could be quite large. So we have to scale each metric -- put them on a similar basis using the distance from the max value as our guide. If this sounds familiar, it is: It's a z-score.

Also, k-means clustering won't work with text data, so we need to create a dataframe that's just the numbers, but scaled. We can do that with another select, and using mutate_all with the scale function. The `na.omit()` means get rid of any blanks, because they too will cause errors. 

```{r cluster2, exercise=TRUE, exercise.setup = "cluster-load-data", message=FALSE}
playersscaled <- playersselected |> 
  select(MP, `FG%`, `3P%`, AST, TOV, PTS) |> 
  mutate_all(scale) |> 
  na.omit()
```
```{r cluster2-solution, exercise.reveal_solution = FALSE}
playersscaled <- playersselected |> 
  select(MP, `FG%`, `3P%`, AST, TOV, PTS) |> 
  mutate_all(scale) |> 
  na.omit()
```
```{r cluster2-check}
grade_this_code()
```

With k-means clustering, we decide how many clusters we want. Most often, researchers will try a handful of different cluster numbers and see what works. But there are methods for finding the optimal number. One method is called the Elbow method. One implementation of this, [borrowed from the University of Cincinnati's Business Analytics program](https://uc-r.github.io/kmeans_clustering), does this quite nicely with a graph that will help you decide for yourself. 

All you need to do in this code is change out the data frame -- `playersscaled` in this case -- and run it. 

```{r cluster3, exercise=TRUE, exercise.setup = "cluster-load-data", message=FALSE}
# function to compute total within-cluster sum of square 
wss <- function(k) {
  kmeans(playersscaled, k, nstart = 10 )$tot.withinss
}

# Compute and plot wss for k = 1 to k = 15
k.values <- 1:15

# extract wss for 2-15 clusters
wss_values <- map_dbl(k.values, wss)

plot(k.values, wss_values,
       type="b", pch = 19, frame = FALSE, 
       xlab="Number of clusters K",
       ylab="Total within-clusters sum of squares")
```
```{r cluster3-solution, exercise.reveal_solution = FALSE}
# function to compute total within-cluster sum of square 
wss <- function(k) {
  kmeans(playersscaled, k, nstart = 10 )$tot.withinss
}

# Compute and plot wss for k = 1 to k = 15
k.values <- 1:15

# extract wss for 2-15 clusters
wss_values <- map_dbl(k.values, wss)

plot(k.values, wss_values,
       type="b", pch = 19, frame = FALSE, 
       xlab="Number of clusters K",
       ylab="Total within-clusters sum of squares")
```
```{r cluster3-check}
grade_this_code()
```

The Elbow method -- so named because you're looking for the "elbow" where the line flattens out. In this case, it looks like a K of 5 is ideal. So let's try that. We're going to use the kmeans function, saving it to an object called clusters. We just need to tell it our dataframe name, how many centers (k) we want, and we'll use a sensible default for how many different configurations to try. 

```{r cluster4, exercise=TRUE, exercise.setup = "cluster-load-data", message=FALSE}
clusters <- kmeans(playersscaled, centers = 5, nstart = 25)
```
```{r cluster4-solution, exercise.reveal_solution = FALSE}
clusters <- kmeans(playersscaled, centers = 5, nstart = 25)
```
```{r cluster4-check}
grade_this_code()
```

Let's look at what we get.

```{r cluster5, exercise=TRUE, exercise.setup = "cluster-load-data", message=FALSE}
clusters
```
```{r cluster5-solution, exercise.reveal_solution = FALSE}
clusters
```
```{r cluster5-check}
grade_this_code()
```

Interpreting this output, the very first thing you need to know is that **the cluster numbers are meaningless**. They aren't ranks. They aren't anything. After you have taken that on board, look at the cluster sizes at the top. What clusters are large compared to others. That's notable. Then we can look at the cluster means. For reference, 0 is going to be average. So which groups are above average on minutes played? Which groups are below? 

So which group is Keisei Tominaga in? Well, first we have to put our data back together again. In clusters, there is a list of cluster assignments in the same order we put them in, but recall we have no names. So we need to re-combine them with our original data. We can do that with the following:

```{r cluster6, exercise=TRUE, exercise.setup = "cluster-load-data", message=FALSE}
playercluster <- data.frame(playersselected, clusters$cluster) 
```
```{r cluster6-solution, exercise.reveal_solution = FALSE}
playercluster <- data.frame(playersselected, clusters$cluster) 
```
```{r cluster6-check}
grade_this_code()
```

Now we have a dataframe called playercluster that has our player names and what cluster they are in. The fastest way to find Keisei Tominaga is to double click on the playercluster table in the environment and use the search in the top right of the table. Because this is based on some random selections of points to start the groupings, these may change from person to person. 

We now have a dataset and can plot it like anything else. Let's get Keisei Tominaga and then plot him against the rest of college basketball on points versus minutes played. 

```{r cluster7, exercise=TRUE, exercise.setup = "cluster-load-data", message=FALSE}
kt <- playercluster |> filter(Player == "Keisei Tominaga")

kt
```
```{r cluster7-solution, exercise.reveal_solution = FALSE}
kt <- playercluster |> filter(Player == "Keisei Tominaga")

kt
```
```{r cluster7-check}
grade_this_code()
```

What does that look like? Let's look at scoring in a chart. 

```{r cluster8, exercise=TRUE, exercise.setup = "cluster-load-data", message=FALSE}
ggplot() + 
  geom_point(data=playercluster, aes(x=MP, y=PTS, color=clusters.cluster)) + 
  geom_point(data=kt, aes(x=MP, y=PTS), color="red")
```
```{r cluster8-solution, exercise.reveal_solution = FALSE}
ggplot() + 
  geom_point(data=playercluster, aes(x=MP, y=PTS, color=clusters.cluster)) + 
  geom_point(data=kt, aes(x=MP, y=PTS), color="red")
```
```{r cluster8-check}
grade_this_code()
```

Not bad, not bad. But who are Keisei Tominaga's peers? If we look at the numbers in his group, how many of them are there? What are their stats relative to average? So let's limit them to just Big Ten guards. Unfortunately, I don't have Conference names in my data, so I'm going to have to do this the hard way and make a list of Big Ten teams and filter on that. Then I'll sort by minutes played. 

```{r cluster9, exercise=TRUE, exercise.setup = "cluster-load-data", message=FALSE}
big10 <- c("Nebraska Cornhuskers Men's", "Iowa Hawkeyes Men's", "Minnesota Golden Gophers Men's", "Illinois Fighting Illini Men's", "Northwestern Wildcats Men's", "Wisconsin Badgers Men's", "Indiana Hoosiers Men's", "Purdue Boilermakers Men's", "Ohio State Buckeyes Men's", "Michigan Wolverines Men's", "Michigan State Spartans Men's", "Penn State Nittany Lions Men's", "Rutgers Scarlet Knights Men's", "Maryland Terrapins Men's")

playercluster |> filter(clusters.cluster == kt$clusters.cluster) |> filter(Team %in% big10) |> arrange(desc(MP))
```
```{r cluster9-solution, exercise.reveal_solution = FALSE}
big10 <- c("Nebraska Cornhuskers Men's", "Iowa Hawkeyes Men's", "Minnesota Golden Gophers Men's", "Illinois Fighting Illini Men's", "Northwestern Wildcats Men's", "Wisconsin Badgers Men's", "Indiana Hoosiers Men's", "Purdue Boilermakers Men's", "Ohio State Buckeyes Men's", "Michigan Wolverines Men's", "Michigan State Spartans Men's", "Penn State Nittany Lions Men's", "Rutgers Scarlet Knights Men's", "Maryland Terrapins Men's")

playercluster |> filter(clusters.cluster == kt$clusters.cluster) |> filter(Team %in% big10) |> arrange(desc(MP))
```
```{r cluster9-check}
grade_this_code()
```

Who are the guards most like Keisei Tominaga in the Big Ten? We should further limit our number by minutes played, but you can do that on your own another time.

## The Recap

In this lesson, we've explored the powerful technique of k-means clustering and its application in sports analytics. We started by preparing our data, selecting relevant statistics for guards in college basketball, and scaling these metrics to ensure fair comparisons. We then learned how to determine the optimal number of clusters using the Elbow method, providing a data-driven approach to grouping players. By applying k-means clustering to our dataset, we were able to group players with similar statistical profiles, focusing on Keisei Tominaga as our example. We visualized these clusters using ggplot2, allowing us to see how players are distributed based on their performance metrics. Finally, we narrowed our focus to Big Ten guards, identifying players most similar to Tominaga within his cluster. This hands-on approach has equipped you with the skills to use clustering for player comparisons, talent evaluation, and strategic analysis in sports. Remember, while clustering is a powerful tool, it's important to combine these insights with domain knowledge and consider the context of the sport when interpreting results.

## Terms to Know

- K-means Clustering: A machine learning algorithm that groups similar data points into a specified number of clusters based on their features.
- Euclidean Distance: The straight-line distance between two points in a multi-dimensional space, used in k-means clustering to measure similarity between data points.
- Cluster: A group of data points in a k-means analysis that are more similar to each other than to data points in other clusters.
- Centroid: The mean point of all data points within a cluster, representing the cluster's center.
- Elbow Method: A technique used to determine the optimal number of clusters (k) by plotting the sum of squared distances against different k values and looking for an "elbow" in the curve.
- Scaling: The process of standardizing variables to a common scale to ensure each feature contributes equally to the clustering analysis.