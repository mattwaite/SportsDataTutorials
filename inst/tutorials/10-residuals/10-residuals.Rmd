---
title: "Sports Data Lesson 10: Residuals"
output: 
  learnr::tutorial:
    progressive: true
    allow_skip: true
    theme: united
    ace_theme: github
runtime: shiny_prerendered
description: >
  Learn how to see who is better than expected.
---

```{r setup, include=FALSE}
library(learnr)
library(gradethis)
library(tidyverse)
library(glue)
knitr::opts_chunk$set(echo = FALSE)
tutorial_options(exercise.completion=FALSE)
```

## The Goal

The goal of this lesson is to introduce you to residuals, a crucial concept in linear regression analysis. You'll learn what residuals are, how to calculate them, and why they're important in evaluating the effectiveness of your regression models. We'll explore how to interpret residuals to identify teams or players who are overperforming or underperforming relative to what the model predicts. You'll also learn how to visualize residuals to check the appropriateness of your linear model and to spot potential patterns or outliers in your data. By the end of this lesson, you'll be able to use residual analysis to enhance your understanding of statistical models in sports analytics, helping you to make more nuanced interpretations of team and player performance beyond what basic statistics might suggest.

## The Basics

When looking at a linear model of your data, there's a measure you need to be aware of called residuals. The residual is the distance between what the model predicted and what the real outcome is.  

Residuals can tell you several things, but most important is if a linear model the right model for your data. If the residuals appear to be random, then a linear model is appropriate. If they have a pattern, it means something else is going on in your data and a linear model isn't appropriate. 

Residuals can also tell you who is underperforming and overperforming the model. And the more robust the model -- the better your r-squared value is -- the more meaningful that label of under or overperforming is. 

Let's look at a net yards model from the last lesson. We use data from the last college football season. First, load the tidyverse.

```{r load-tidyverse, exercise=TRUE}
library(tidyverse)
```
```{r load-tidyverse-solution}
library(tidyverse)
```
```{r load-tidyverse-check}
grade_this_code()
```

Now import the data.

```{r correlations-load-data, message=FALSE, warning=FALSE, results=FALSE}
correlations <- read_csv("https://mattwaite.github.io/sportsdatafiles/footballlogs24.csv")

residualmodel <- correlations |> 
  mutate(
    differential = TeamScore - OpponentScore,
    NetYards = OffensiveYards - DefYards
    )

fit <- lm(differential ~ NetYards, data = residualmodel)
summary(fit)

penalties <- correlations |> 
  mutate(
    differential = TeamScore - OpponentScore, 
    TotalPenalties = Penalties+DefPenalties,
    TotalPenaltyYards = PenaltyYds+DefPenaltyYds
  )

pfit <- lm(differential ~ TotalPenaltyYards, data = penalties)
summary(pfit)

penresid <- penalties |> mutate(predicted = predict(pfit), residuals = residuals(pfit))

pensort <- penresid |> arrange(desc(residuals)) |> select(Team, Opponent, Result, differential, TotalPenaltyYards, predicted, residuals) |> slice(1)
```
```{r correlations-load-data-exercise, exercise = TRUE}
correlations <- read_csv("https://mattwaite.github.io/sportsdatafiles/footballlogs24.csv")
```
```{r correlations-load-data-exercise-solution}
correlations <- read_csv("https://mattwaite.github.io/sportsdatafiles/footballlogs24.csv")
```
```{r correlations-load-data-exercise-check}
grade_this_code()
```

Under the hood, I've gone ahead and made the mutations for you. You can see it in a dataframe called residualmodel.

```{r head-data, exercise=TRUE, exercise.setup = "correlations-load-data"}
head(residualmodel)
```
```{r head-data-solution}
head(residualmodel)
```
```{r head-data-check}
grade_this_code()
```

### Exercise 1: Create the model.

Just like the headline says, it's time to create a linear model like we did in the last lesson. We'll create a linear model object called fit, and we'll use our residualmodel dataframe. In the model, we'll say that differential is approximately modeled by NetYards.

```{r regression1, exercise=TRUE, exercise.setup = "correlations-load-data"}
____ <- lm(differential ~ ____, data = ____)
summary(____)
```
```{r regression1-solution, exercise.reveal_solution = FALSE}
fit <- lm(differential ~ NetYards, data = residualmodel)
summary(fit)
```
```{r regression1-check}
grade_this_code()
```

```{r residuals1, exercise=FALSE, exercise.eval=TRUE, exercise.setup = "correlations-load-data", results='asis'}
glue("We've seen this output before, but let's review because if you are using scatterplots to make a point, you should do this. First, note the Min and Max residual at the top. A team has underperformed the model by nearly {format(min(fit$residuals),digits=1)} points (!), and a team has overperformed it by {format(max(fit$residuals),digits=1)} points (!!). The median residual, where half are above and half are below, is just slightly below the fit line. Close here is good.
     
Next: Look at the Adjusted R-squared value. What that says is that {format(summary(fit)$r.squared*100, digits=1)} percent of a team's scoring output can be predicted by their net yards.")
```

Last: Look at the p-value. We are looking for a p-value smaller than .05. At .05, we can say that our correlation didn't happen at random. And, in this case, it REALLY didn't happen at random. But if you know a little bit about football, it doesn't surprise you that the more you outgain your opponent, the more you win by. It's an intuitive result.

What we want to do now is look at those residuals. We want to add the residuals to our individual game records. We can do that by creating two new fields -- predicted and residuals -- to our dataframe. We're going to use a two functions in mutate you haven't seen yet -- predict and residuals. Predict will give you the number our model would predict would happen given our model. Residuals does the work for you by taking the predicted score and subtracting the actual score. A positive number overshoots the model. A negative number undershoots it.

Then, let's arrange it by residuals so see see the worst misses, and then use select to make it all easier to read.

```{r regression2, exercise=TRUE, exercise.setup = "correlations-load-data"}
residualmodel <- residualmodel |> mutate(predicted = predict(fit), residuals = residuals(fit))

residualmodel |> 
  arrange(desc(residuals)) |> 
  select(Team, Opponent, Result, differential, NetYards, predicted, residuals)
```
```{r regression2-solution, exercise.reveal_solution = FALSE}
residualmodel <- residualmodel |> mutate(predicted = predict(fit), residuals = residuals(fit))

residualmodel |> 
  arrange(desc(residuals)) |> 
  select(Team, Opponent, Result, differential, NetYards, predicted, residuals)
```
```{r regression2-check}
grade_this_code()
```

Looking at this table, what you see here are the teams who scored more than their net yards would indicate.

But, before we can bestow any validity on this model, we need to see if this linear model is appropriate. We've done that some looking at our p-values and R-squared values. But one more check is to look at the residuals themselves. We do that by plotting the residuals with the predictor. We'll get into plotting soon, but for now just seeing it is enough.

```{r correlationchart, exercise=FALSE, exercise.eval=TRUE, exercise.setup = "correlations-load-data"}
residualmodel <- residualmodel |> mutate(predicted = predict(fit), residuals = residuals(fit))

ggplot(residualmodel, aes(x=NetYards, y=residuals)) + geom_point()
```

The lack of a shape here -- the seemingly random nature -- is a good sign that a linear model works for our data. If there was a pattern, that would indicate something else was going on in our data and we needed a different model.

Another way to view your residuals is by connecting the predicted value with the actual value.

```{r correlationchart2, exercise=FALSE, exercise.eval=TRUE, exercise.setup = "correlations-load-data"}
residualmodel <- residualmodel |> mutate(predicted = predict(fit), residuals = residuals(fit))

ggplot(data=residualmodel, aes(x=NetYards, y=differential)) + geom_point() + geom_segment(aes(xend = NetYards, yend = predicted)) + geom_smooth(method=lm, se=FALSE)
```

The blue line here separates underperformers from overperformers.

## Penalties

Now let's look at it where it doesn't work: Penalties. 

Under the hood, I've gone ahead and made the mutations for you from the last chapter. You can see it in a dataframe called penalties

```{r phead-data, exercise=TRUE, exercise.setup = "correlations-load-data"}
head(penalties)
```
```{r phead-data-solution}
head(penalties)
```
```{r phead-data-check}
grade_this_code()
```

### Exercise 2: Create another model.

For this we'll make an object called pfit -- for penalty fit -- similar to what we did before. This time, we'll say differential is approximately modeled by TotalPenaltyYards. 

```{r regression3, exercise=TRUE, exercise.setup = "correlations-load-data"}
pfit <- lm(____ ~ ____, data = penalties)
summary(____)
```
```{r regression3-solution, exercise.reveal_solution = FALSE}
pfit <- lm(differential ~ TotalPenaltyYards, data = penalties)
summary(pfit)
```
```{r regression3-check}
grade_this_code()
```

```{r residuals2, exercise=FALSE, exercise.eval=TRUE, exercise.setup = "correlations-load-data", results='asis'}
glue("So from top to bottom:

* Our min and max go from {format(min(pfit$residuals),digits=1)} to positive {format(max(pfit$residuals),digits=1)}
* Our adjusted R-squared is ... {format(summary(pfit)$r.squared, digits=5)}. Not much at all. 
* Our p-value is ... {summary(pfit)$coefficients[, \"Pr(>|t|)\"][2]}, which is more than than .05.")
```

So what we can say about this model is that it's statistically insignificant and utterly meaningless. Normally, we'd stop right here -- why bother going forward with a predictive model that isn't predictive? But let's do it anyway. 

### Exercise 3: Adding residuals and arranging them.

Just like a previous exercise, we'll add predicted and residuals to our dataframe. Then we'll arrange it, and use a little select magic to make it easier to read. 

```{r regression4, exercise=TRUE, exercise.setup = "correlations-load-data"}
penalties <- penalties |> mutate(predicted = predict(____), residuals = ____(pfit))

____ |> arrange(desc(____)) |> select(Team, Opponent, Result, differential, TotalPenaltyYards, predicted, residuals)
```
```{r regression4-solution, exercise.reveal_solution = FALSE}
penalties <- penalties |> mutate(predicted = predict(pfit), residuals = residuals(pfit))

penalties |> arrange(desc(residuals)) |> select(Team, Opponent, Result, differential, TotalPenaltyYards, predicted, residuals)
```
```{r regression4-check}
grade_this_code()
```

```{r residuals3, exercise=FALSE, exercise.eval=TRUE, exercise.setup = "correlations-load-data", results='asis'}
glue("First, note all of the biggest misses here are all blowout games. The worst games of the season, the worst being {pensort$Team} vs {pensort$Opponent}. The model missed that differential by ... {format(pensort$residuals, digits=1)} points. The margin of victory? {format(pensort$differential, digits=1)} points. In other words, this model is terrible. But let's look at it anyway.")
```

```{r correlationchart3, exercise=FALSE, exercise.eval=TRUE, exercise.setup = "correlations-load-data"}
penalties <- penalties |> mutate(predicted = predict(pfit), residuals = residuals(pfit))

ggplot(penalties, aes(x=TotalPenaltyYards, y=residuals)) + geom_point()
```

Well ... it actually says that a linear model is appropriate. Which an important lesson -- just because your residual plot says a linear model works here, that doesn't say your linear model is good. There are other measures for that, and you need to use them. 

Here's the segment plot of residuals -- you'll see some really long lines. That's a bad sign. Another bad sign? A flat fit line. It means there's no relationship between these two things. Which we already know.

```{r correlationchart4, exercise=FALSE, exercise.eval=TRUE, exercise.setup = "correlations-load-data"}
penalties <- penalties |> mutate(predicted = predict(fit), residuals = residuals(fit))

ggplot(data=penalties, aes(x=TotalPenaltyYards, y=differential)) + geom_point() + geom_segment(aes(xend = TotalPenaltyYards, yend = predicted)) + geom_smooth(method=lm, se=FALSE)
```

## The Recap

In this lesson, we've delved into the world of residuals, a key component of linear regression analysis. We learned that residuals represent the difference between observed values and those predicted by our model, providing valuable insights into model performance and data patterns. We practiced calculating residuals using R, interpreting them to identify over- and underperforming teams, and visualizing them to assess model appropriateness. Through our examples with college football data, we saw how residual analysis can reveal nuances in team performance that simple statistics might miss. We also encountered a case where residuals indicated a linear model was appropriate, but other metrics showed the model wasn't meaningful, highlighting the importance of considering multiple factors when evaluating models. Remember, while residuals are powerful tools, they should be used in conjunction with other statistical measures and domain knowledge for comprehensive analysis. 

## Terms to Know

- Residuals: The differences between observed values and values predicted by a statistical model.
- Linear Model: A statistical model that assumes a linear relationship between variables, often used in regression analysis.
- Overperforming: When an observed value is higher than what the model predicts, resulting in a positive residual.
- Underperforming: When an observed value is lower than what the model predicts, resulting in a negative residual.
- predict(): An R function used to generate predicted values from a fitted model.
- residuals(): An R function used to extract the residuals from a fitted model.
- Model Fit: How well a statistical model describes the relationship in the data, often assessed using residuals and other metrics.
- Outliers: Data points that differ significantly from other observations, often identified through residual analysis.